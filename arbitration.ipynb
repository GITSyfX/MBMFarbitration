{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd958a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_, max_ = 1e-12, 1e12\n",
    "sigmoid = lambda x: 1.0 / (1.0 + clip_exp(-x))\n",
    "logit   = lambda p: np.log(p) - np.log1p(-p)\n",
    "\n",
    "def clip_exp(x):\n",
    "    x = np.clip(x, a_min=-max_, a_max=50)\n",
    "    y = np.exp(x)\n",
    "    return np.where(y > 1e-11, y, 0)\n",
    "\n",
    "class simpleBuffer:\n",
    "    '''Simple Buffer 2.0\n",
    "    Update log: \n",
    "        To prevent naive writing mistakes,\n",
    "        we turn the list storage into dict.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.m = {}\n",
    "        \n",
    "    def push(self, m_dict):\n",
    "        self.m = {k: m_dict[k] for k in m_dict.keys()}\n",
    "        \n",
    "    def sample(self, *args):\n",
    "        lst = [self.m[k] for k in args]\n",
    "        if len(lst)==1: return lst[0]\n",
    "        else: return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    name = 'Model Free'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "\n",
    "    gamma = 1 \n",
    "    \n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self.gamma = MF.gamma\n",
    "        self._init_mem()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "\n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0] # learning rate \n",
    "        self.beta  = params[1] # inverse temperature \n",
    "\n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        #print(logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "        # ----------- learning ----------- #\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, s_next,a_next, r, done = self.mem.sample(\n",
    "                        's','a','s_next','a_next','r','done')\n",
    "        \n",
    "        if done != True:  \n",
    "            self.RPE = r + self.gamma*self.Q[s_next,a_next]-self.Q[s,a]\n",
    "        else:\n",
    "            self.RPE = r - self.Q[s,a]\n",
    "        # Q-update\n",
    "        self.Q_old = self.Q[s,a]\n",
    "        self.Q[s,a] = self.Q[s,a]+self.alpha*self.RPE\n",
    "\n",
    "        return self.RPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g):\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MB():\n",
    "    name = 'Model Based raw'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "    \n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self._init_mem()\n",
    "        self._init_env_model()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "    \n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0]\n",
    "        self.beta  = params[1]\n",
    "    \n",
    "    def _init_env_model(self):\n",
    "        self.T_bel = np.zeros(\n",
    "            [self.env.nS, self.env.nA, self.env.nS] #(s,a,s')\n",
    "            ) \n",
    "        for s in range(self.env.nS):\n",
    "            for a in range(self.env.nA):\n",
    "                self.T_bel[s,a,:] = 1/self.env.nS\n",
    "        \n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    # ----------- learning ----------- #\n",
    "    def learn(self):\n",
    "\n",
    "        s, a, s_next, r, g = self.mem.sample(\n",
    "                        's','a','s_next','r','g')\n",
    "        \n",
    "        self.env.set_reward(g)\n",
    "        \n",
    "        if s < 5: \n",
    "            self.SPE = 1-self.T_bel[s,a,s_next]\n",
    "            # T-update (increase T(s,a,s') and decrease T(s,a,-) to ensure the sum=1\n",
    "            self.T_bel[s,a,s_next]= self.T_bel[s,a,s_next] + self.alpha * self.SPE\n",
    "            array_rest = np.where(np.arange(self.env.nS) != s_next)[0] #the rest of the states\n",
    "            for j in array_rest:\n",
    "                self.T_bel[s,a,j] = self.T_bel[s,a,j]*(1-self.alpha) #SPE = 0-self.T\n",
    "\n",
    "\n",
    "            # compute the prediction error \n",
    "            Q_sum=0\n",
    "            Q_sum=Q_sum+self.T_bel[s,a,s_next]*(r+max(self.Q[s_next,:])) # for (s,a,s')\n",
    "            \n",
    "            for j in array_rest: # for the rest\n",
    "                Q_sum=Q_sum+self.T_bel[s,a,j]*(self.env.R[j]+max(self.Q[j,:]))\n",
    "                \n",
    "            self.Q_old = self.Q[s,a]\n",
    "            self.Q[s,a] = Q_sum\n",
    "        return self.SPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g): #update all Q(s) when switch reward target\n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        Q_bwd_before = self.Q\n",
    "        for i in range(max(self.env.level),0,-1):\n",
    "            state_ind_set = np.where(self.env.level==i)\n",
    "            for l in range(len(state_ind_set)):\n",
    "                current_S = state_ind_set[l]\n",
    "                for current_A in range(self.env.nA):\n",
    "                    tmp_sum=0\n",
    "                    for j in range(self.env.nS): \n",
    "                        tmp_sum = tmp_sum + self.T_bel[current_S,current_A,j] * \\\n",
    "                                (self.env.R[j]+max(self.Q[j,:]))\n",
    "                    self.Q[current_S,current_A]=tmp_sum\n",
    "        Q_bwd_after = self.Q\n",
    "        self.dQ = Q_bwd_after - Q_bwd_before\n",
    "        self.dQ_bwd_energy = np.sqrt(np.sum(np.sum((self.dQ)**2)))\n",
    "        self.dQ_mean_energy = np.mean(np.mean(self.dQ))\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MB4MDT():\n",
    "    name = 'Model Based'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "\n",
    "\n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self._init_mem()\n",
    "        self._init_env_model()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "    \n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0]\n",
    "        self.beta  = params[1]\n",
    "    \n",
    "    def _init_env_model(self):\n",
    "        self.T_bel = np.zeros(\n",
    "            [self.env.nS, self.env.nA, self.env.nS] #(s,a,s')\n",
    "            ) \n",
    "        self.T = {0:[[1,2], [6,7], [7,8], [6,5], [6,8]],\n",
    "                  1:[[3,4], [7,8], [6,8], [5,8], [8,5]]}\n",
    "\n",
    "        for a in range(self.env.nA):\n",
    "            for s in range(len(self.T[a])):\n",
    "                self.T_bel[s,a,self.T[a][s]]=[0.5,1-0.5]        \n",
    "        \n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    # ----------- learning ----------- #\n",
    "    def learn(self):\n",
    "\n",
    "        s, a, s_next, r, g = self.mem.sample(\n",
    "                        's','a','s_next','r','g')\n",
    "        \n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        if s < 5: \n",
    "            self.SPE=1-self.T_bel[s,a,s_next]\n",
    "            # T-update (increase T(s,a,s') and decrease T(s,a,-) to ensure the sum=1\n",
    "            self.T_bel[s,a,s_next]= self.T_bel[s,a,s_next] + self.alpha * self.SPE\n",
    "            s_unchosen = [elem for elem in self.T[a][s] if elem != s_next] # the rest of the states\n",
    "            for j in s_unchosen:\n",
    "                self.T_bel[s,a,j] = self.T_bel[s,a,j]*(1-self.alpha) #SPE = 0-self.T\n",
    "\n",
    "            # Q update\n",
    "            Q_sum=0\n",
    "            Q_sum=Q_sum+self.T_bel[s,a,s_next]*(r+max(self.Q[s_next,:])) # for (s,a,s')\n",
    "            \n",
    "            for j in s_unchosen: # for the rest\n",
    "                Q_sum=Q_sum+self.T_bel[s,a,j]*(self.env.R[j]+max(self.Q[j,:]))\n",
    "                \n",
    "            self.Q_old = self.Q[s,a] \n",
    "            self.Q[s,a]= Q_sum\n",
    "\n",
    "            #print(SPE, s, a, r, self.alpha, self.Q_old, self.Q[s, a])\n",
    "        return self.SPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g): #update all Q(s) when switch reward target\n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        Q_bwd_before = self.Q\n",
    "        for i in range(max(self.env.level),0,-1):\n",
    "            state_ind_set = np.where(self.env.level==i)\n",
    "            for l in range(len(state_ind_set)):\n",
    "                current_S = state_ind_set[l]\n",
    "                for current_A in range(self.env.nA):\n",
    "                    tmp_sum=0\n",
    "                    for j in range(self.env.nS): \n",
    "                        tmp_sum = tmp_sum + self.T_bel[current_S,current_A,j] * \\\n",
    "                                (self.env.R[j]+max(self.Q[j,:]))\n",
    "                    self.Q[current_S,current_A]=tmp_sum\n",
    "        Q_bwd_after = self.Q\n",
    "        self.dQ = Q_bwd_after - Q_bwd_before\n",
    "        self.dQ_bwd_energy = np.sqrt(np.sum(np.sum((self.dQ)**2)))\n",
    "        self.dQ_mean_energy = np.mean(np.mean(self.dQ))\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDT:\n",
    "    name = 'MixedArb-Dynamic'\n",
    "    bnds = [(1e-3, 1), (0, 1), (0.02, 10), (0.02, 10), (0, 1), (0, 5)]\n",
    "    pbnds = [(0.3, 0.7), (0.1, 0.35), (0.02, 5), (0.02, 5), (0.1, 0.5), (0.1, 2)]\n",
    "    p_name = ['w','eta','A_F2B','A_B2F','alpha','beta'] #参数名\n",
    "    n_params = len(bnds) \n",
    "\n",
    "    p_trans = [\n",
    "        \n",
    "        lambda x: 1e-3 + (1 - 1e-3) * sigmoid(x),     # (1e-3, 1)\n",
    "        lambda x: 0  + (1 - 0.0)  * sigmoid(x),     # (0, 1)\n",
    "        lambda x: 0.02  + (10 - 0.02) * sigmoid(x),     # (0.02, 10)\n",
    "        lambda x: 0.02 + (10 - 0.02)* sigmoid(x),     # (0.02, 10)\n",
    "        lambda x: 0.0  + (1 - 0.0)  * sigmoid(x),     # (0, 1)\n",
    "        lambda x: 0.0  + (5 - 0.0)  * sigmoid(x),     # (0, 5)\n",
    "    ]\n",
    "    \n",
    "    p_links = [\n",
    "        lambda y: logit(np.clip((y - 1e-3) / (1 - 1e-3), eps_, 1 - eps_)),\n",
    "        lambda y: logit(np.clip((y - 0.0 ) / (1 - 0.0 ), eps_, 1 - eps_)),\n",
    "        lambda y: logit(np.clip((y - 0.02 ) / (10 - 0.02), eps_, 1 - eps_)),\n",
    "        lambda y: logit(np.clip((y - 0.02) / (10 - 0.02), eps_, 1 - eps_)),\n",
    "        lambda y: logit(np.clip((y - 0.0 ) / (1 - 0.0 ), eps_, 1 - eps_)),\n",
    "        lambda y: logit(np.clip((y - 0.0 ) / (5 - 0.0 ), eps_, 1 - eps_)),\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, env,params):\n",
    "        self.env = env\n",
    "        self._load_params(params)\n",
    "        self._load_agents()\n",
    "        self._init_mem()\n",
    "        self._init_critic()\n",
    "        self._init_PEest()\n",
    "        self._init_arbitration()\n",
    "        self._init_Qinteg()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.w = params[0] # PE tolerance\n",
    "        self.eta = params[1] # learning rate of absolute PE estimation \n",
    "        self.A_F2B = params[2]\n",
    "        self.A_B2F = params[3]\n",
    "        self.alpha = params[4]\n",
    "        self.beta = params[5]\n",
    "\n",
    "    def _load_agents(self):\n",
    "        self.agent_MF = MF(self.env,params=[self.alpha,self.beta])\n",
    "        self.agent_MB = MB(self.env,params=[self.alpha,self.beta])\n",
    "\n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "        \n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "            \n",
    "    def _init_PEest(self):\n",
    "\n",
    "        '''Beyesian Reliability estimation'''\n",
    "        self.K=3; # trichonomy of PE\n",
    "        self.M=19; # memory size = 10\n",
    "        self.M_half=6; # half-life\n",
    "        self.M_current_MB=0; # MF accumulated events. cannot exceed T.\n",
    "    \n",
    "        # PE history \n",
    "        # row: first index = smallest PE -> last index - largest PE\n",
    "        # column: first index = most recent -> last index = most past\n",
    "        self.MB_PE_history = np.zeros((self.K,self.M))\n",
    "        self.MB_PE_num = np.zeros((self.K,1))   \n",
    "\n",
    "        #self.discount_mat = np.exp((-1)*log(2)*([0:1:self.T-1]/(self.T_half-1))); #prepare discount mat: 1xT\n",
    "        #self.discount_mat = np.ones((1,self.M)); # no discount\n",
    "    \n",
    "        '''Pearce-Hall associability'''\n",
    "        # Bayesian part - MF RPE estimator\n",
    "        self.MF_absPEestimate = 0.0\n",
    "\n",
    "        # Bayesian part - mean,var,inv_Fano\n",
    "        self.MB_mean = 1/3*np.ones((self.K,1))\n",
    "        self.MF_mean = 1/3*np.ones((self.K,1))\n",
    "\n",
    "        self.MB_var = (2/((3^2)*4))*np.ones((self.K,1))  \n",
    "        self.MF_var = (2/((3^2)*4))*np.ones((self.K,1))\n",
    "        \n",
    "        self.MB_inv_Fano = (self.MB_mean)/self.MB_var\n",
    "        self.MF_inv_Fano = (self.MF_mean)/self.MF_var\n",
    "\n",
    "    def _init_arbitration(self):\n",
    "        # weights for {-PE(to be unlearned), 0PE, +PE(to be learned)}\n",
    "        self.ind_active_model = 1\n",
    "        self.time_step = 1\n",
    "\n",
    "        self.B_B2F = np.log(np.maximum(self.A_B2F / 0.01 - 1, 1e-10))\n",
    "        self.B_F2B = np.log(np.maximum(self.A_F2B / 0.01 - 1, 1e-10))\n",
    "\n",
    "        if self.ind_active_model == 1:\n",
    "            self.MB_prob_prev=0.7   \n",
    "        else:\n",
    "            self.MB_prob_prev=0.3\n",
    "\n",
    "        self.MF_prob_prev = 1-self.MB_prob_prev\n",
    "        self.MB_prob = self.MB_prob_prev\n",
    "        self.MF_prob = self.MF_prob_prev\n",
    "\n",
    "        if self.ind_active_model == 1:\n",
    "            self.num_MB_chosen=1\n",
    "            self.num_MF_chosen=0\n",
    "        else:\n",
    "            self.num_MB_chosen=0\n",
    "            self.num_MF_chosen=1\n",
    "\n",
    "\n",
    "    def _init_Qinteg(self):\n",
    "        #Q-integration part\n",
    "        self.p = 1 # 1:expectation, 1e1:winner-take-all\n",
    "        # set the first transition rate to be the equilibrium rate\n",
    "        if self.B_F2B != self.B_B2F:\n",
    "            self.inv_Fano_equilibrium = np.log(self.A_F2B/self.A_B2F)/(self.B_F2B-self.B_B2F) # applied only for the unnormalized case\n",
    "            # self.inv_Fano_equilibrium=.5;\n",
    "            self.transition_rateB2F_prev = self.A_B2F*np.exp((-1)*self.B_B2F*self.inv_Fano_equilibrium)\n",
    "            self.transition_rateF2B_prev = self.transition_rateB2F_prev\n",
    "        else:\n",
    "            # self.inv_Fano_equilibrium=150;\n",
    "            self.inv_Fano_equilibrium =.1\n",
    "            # self.transition_rateB2F_prev=self.A_B2F*exp((-1)*self.B_B2F*self.inv_Fano_equilibrium);\n",
    "            self.transition_rateB2F_prev = self.inv_Fano_equilibrium\n",
    "            self.transition_rateF2B_prev = self.transition_rateB2F_prev\n",
    "\n",
    "        self.transition_rateB2F = self.transition_rateB2F_prev\n",
    "        self.transition_rateF2B = self.transition_rateF2B_prev\n",
    "\n",
    "    def eval_act(self,s,a):\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "\n",
    "    def policy(self, s):\n",
    "        logit = self.Q[s,:]\n",
    "        return softmax(self.beta*logit)\n",
    "\n",
    "    def beyesion_relest(self):\n",
    "        self.MB_thr_PE = self.w*np.array([-1, 1]) # length = self.K-1\n",
    "\n",
    "        ''' MB model reliability estitation'''  \n",
    "        self.M_current_MB = np.min([self.M_current_MB+1, self\n",
    "                                    .M]) # update # of accumulated events\n",
    "        \n",
    "        # (0) backup old values\n",
    "        self.MB_mean_old = self.MB_mean\n",
    "        self.MB_var_old = self.MB_var    \n",
    "        self.MB_inv_Fano_old = self.MB_inv_Fano\n",
    "        \n",
    "        # (1) find the corresponding row\n",
    "        PE_level = np.where((self.MB_thr_PE-self.agent_MB.SPE)<0); # [!!] must be fwd because it looks into SPE.    \n",
    "        PE_theta = len(PE_level[0]); # 0:neg, 1:zero, 2:posPE\n",
    "        \n",
    "        # (2) update the current column(=1) in PE_history\n",
    "        self.MB_PE_history[:,1:] = self.MB_PE_history[:,0:-1] # shift 1 column (toward past)\n",
    "        self.MB_PE_history[:,0] = np.zeros(self.K) # empty the first column\n",
    "        self.MB_PE_history[PE_theta,0] = 1 # add the count 1 in the first column\n",
    "        self.MB_PE_num = np.sum(self.MB_PE_history == 1, axis=1)  # compute discounted accumulated PE\n",
    "        \n",
    "        # (3) posterior mean & var\n",
    "        sumEvents = np.sum(self.MB_PE_num)\n",
    "        sumEvents_excl = sumEvents-self.MB_PE_num\n",
    "        self.MB_mean = (1+self.MB_PE_num)/(3+sumEvents)\n",
    "        self.MB_var = ((1+self.MB_PE_num)*(2+sumEvents_excl))/((3+sumEvents)**2*(4+sumEvents)) \n",
    "\n",
    "        # (4) caculate reliability\n",
    "        self.MB_triinv_Fano = self.MB_mean/self.MB_var\n",
    "        self.MB_inv_Fano = self.MB_triinv_Fano[1]/sum(self.MB_triinv_Fano) \n",
    "\n",
    "        ''' MF model reliability estitation''' \n",
    "        # (0) backup old values\n",
    "\n",
    "        self.MF_inv_Fano_old = self.MF_inv_Fano\n",
    "\n",
    "        # (1) update of the absolute RPE estimator\n",
    "        self.MF_absPEestimate = self.MF_absPEestimate+self.eta*(abs(self.agent_MF.RPE)-self.MF_absPEestimate)\n",
    "\n",
    "        # (2) caculate reliability\n",
    "        self.MF_inv_Fano = (40-self.MF_absPEestimate)/40 # [0,1]\n",
    "\n",
    "    def Dynamic_Arbit(self): \n",
    "        input1 = self.MB_inv_Fano\n",
    "        input2 = self.MF_inv_Fano\n",
    "        \n",
    "        self.transition_rateF2B = self.A_F2B/(1+np.exp(self.B_F2B*input2))\n",
    "        self.transition_rateB2F = self.A_B2F/(1+np.exp(self.B_B2F*input1))\n",
    "\n",
    "        self.transition_rateF2B_prev = self.transition_rateF2B\n",
    "        self.transition_rateB2F_prev = self.transition_rateB2F\n",
    "\n",
    "\n",
    "        self.tau = 1/(self.transition_rateF2B + self.transition_rateB2F) # alpha + beta term.\n",
    "        self.MB_prob_prev = self.MB_prob\n",
    "        self.MB_prob_inf = self.transition_rateF2B*self.tau\n",
    "        self.MB_prob = self.MB_prob_inf+(self.MB_prob_prev-self.MB_prob_inf)*np.exp((-1)*self.time_step/self.tau)\n",
    "        self.MF_prob = 1-self.MB_prob\n",
    "\n",
    "        # choice of the model\n",
    "        self.ind_active_model_prev = self.ind_active_model\n",
    "        if self.MB_prob > 0.5:\n",
    "            self.ind_active_model = 1\n",
    "            self.num_MB_chosen = self.num_MB_chosen+1\n",
    "            # there is no Q-value hand-over because sarsa computes Q based on SPE.\n",
    "        else:\n",
    "            self.ind_active_model = 2   \n",
    "            self.num_MF_chosen = self.num_MF_chosen+1\n",
    "            # Q-value hand-over : sarsa uses RPE-based Q only, does not use SPE.\n",
    "        \n",
    "    def learn(self): \n",
    "        self.agent_MF.mem = self.mem\n",
    "        self.agent_MB.mem = self.mem\n",
    "        _,_,self.Q_MF = self.agent_MF.learn() \n",
    "        _,_,self.Q_MB = self.agent_MB.learn() \n",
    "        self.beyesion_relest()\n",
    "        self.Dynamic_Arbit()\n",
    "        self.Q = ((self.MB_prob*self.Q_MB)**self.p + (self.MF_prob*self.Q_MF)**self.p)**(1/self.p)\n",
    "    \n",
    "    def bw_update(self,g): \n",
    "        self.Q_MB = self.agent_MB.bw_update(g)\n",
    "        self.Q_MF = self.agent_MF.bw_update(g)\n",
    "        self.Q = ((0.9*self.Q_MB)**self.p + (0.1*self.Q_MF)**self.p)**(1/self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid():\n",
    "    name = 'Hybrid MF-MB'\n",
    "    bnds = [(0,1),(0,5),(0,1)] # 边界\n",
    "    pbnds = [(.1,.5),(.1,2),(.1,.9)] # 采样边界\n",
    "    p_name   = ['alpha', 'beta', 'omega']  # 参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   # alpha\n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x),   # beta\n",
    "               lambda x: 0.0 + (1 - 0.0) * sigmoid(x)]   # omega (MF-MB weight)\n",
    "    \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_)),\n",
    "               lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_))] \n",
    "\n",
    "    gamma = 1 \n",
    "    \n",
    "    def __init__(self, env, params):\n",
    "        self.env = env \n",
    "        self.gamma = Hybrid.gamma\n",
    "        self._init_mem()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "        self._load_agents()\n",
    "\n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0]  # shared learning rate for both MF and MB\n",
    "        self.beta  = params[1]  # inverse temperature \n",
    "        self.omega = params[2]  # weight between MF and MB (0=pure MF, 1=pure MB)\n",
    "\n",
    "    def _load_agents(self):\n",
    "        self.agent_MF = MF(self.env,params=[self.alpha,self.beta])\n",
    "        self.agent_MB = MB(self.env,params=[self.alpha,self.beta])\n",
    "\n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "\n",
    "    # ----------- decision ----------- #\n",
    "    def policy(self, s):\n",
    "        logit = self.Q[s, :]\n",
    "        return softmax(self.beta*logit)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action'''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    # ----------- learning ----------- #\n",
    "    \n",
    "    def learn(self):\n",
    "        self.agent_MF.mem = self.mem\n",
    "        self.agent_MB.mem = self.mem\n",
    "        _,_,self.Q_MF = self.agent_MF.learn() \n",
    "        _,_,self.Q_MB = self.agent_MB.learn() \n",
    "        self.Q = self.omega*self.Q_MB + (1 - self.omega)*self.Q_MF\n",
    "\n",
    "    def bw_update(self, g):\n",
    "        self.Q_MB = self.agent_MB.bw_update(g)\n",
    "        self.Q_MF = self.agent_MF.bw_update(g)\n",
    "        self.Q = 0.9*self.Q_MB + 0.1*self.Q_MF\n",
    "        return self.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
