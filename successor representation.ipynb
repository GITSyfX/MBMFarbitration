{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd958a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_, max_ = 1e-12, 1e12\n",
    "sigmoid = lambda x: 1.0 / (1.0 + clip_exp(-x))\n",
    "logit   = lambda p: np.log(p) - np.log1p(-p)\n",
    "\n",
    "def clip_exp(x):\n",
    "    x = np.clip(x, a_min=-max_, a_max=50)\n",
    "    y = np.exp(x)\n",
    "    return np.where(y > 1e-11, y, 0)\n",
    "\n",
    "class simpleBuffer:\n",
    "    '''Simple Buffer 2.0\n",
    "    Update log: \n",
    "        To prevent naive writing mistakes,\n",
    "        we turn the list storage into dict.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.m = {}\n",
    "        \n",
    "    def push(self, m_dict):\n",
    "        self.m = {k: m_dict[k] for k in m_dict.keys()}\n",
    "        \n",
    "    def sample(self, *args):\n",
    "        lst = [self.m[k] for k in args]\n",
    "        if len(lst)==1: return lst[0]\n",
    "        else: return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    name = 'Model Free'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "\n",
    "    gamma = 1 \n",
    "    \n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self.gamma = MF.gamma\n",
    "        self._init_mem()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "\n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0] # learning rate \n",
    "        self.beta  = params[1] # inverse temperature \n",
    "\n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        #print(logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "        # ----------- learning ----------- #\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, s_next,a_next, r, done = self.mem.sample(\n",
    "                        's','a','s_next','a_next','r','done')\n",
    "        \n",
    "        if done != True:  \n",
    "            self.RPE = r + self.gamma*self.Q[s_next,a_next]-self.Q[s,a]\n",
    "        else:\n",
    "            self.RPE = r - self.Q[s,a]\n",
    "        # Q-update\n",
    "        self.Q_old = self.Q[s,a]\n",
    "        self.Q[s,a] = self.Q[s,a]+self.alpha*self.RPE\n",
    "\n",
    "        return self.RPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g):\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MB():\n",
    "    name = 'Model Based raw'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "    \n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self._init_mem()\n",
    "        self._init_env_model()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "    \n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0]\n",
    "        self.beta  = params[1]\n",
    "    \n",
    "    def _init_env_model(self):\n",
    "        self.T_bel = np.zeros(\n",
    "            [self.env.nS, self.env.nA, self.env.nS] #(s,a,s')\n",
    "            ) \n",
    "        for s in range(self.env.nS):\n",
    "            for a in range(self.env.nA):\n",
    "                self.T_bel[s,a,:] = 1/self.env.nS\n",
    "        \n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    # ----------- learning ----------- #\n",
    "    def learn(self):\n",
    "\n",
    "        s, a, s_next, r, g = self.mem.sample(\n",
    "                        's','a','s_next','r','g')\n",
    "        \n",
    "        self.env.set_reward(g)\n",
    "        \n",
    "        if s < 5: \n",
    "            self.SPE = 1-self.T_bel[s,a,s_next]\n",
    "            # T-update (increase T(s,a,s') and decrease T(s,a,-) to ensure the sum=1\n",
    "            self.T_bel[s,a,s_next]= self.T_bel[s,a,s_next] + self.alpha * self.SPE\n",
    "            array_rest = np.where(np.arange(self.env.nS) != s_next)[0] #the rest of the states\n",
    "            for j in array_rest:\n",
    "                self.T_bel[s,a,j] = self.T_bel[s,a,j]*(1-self.alpha) #SPE = 0-self.T\n",
    "\n",
    "\n",
    "            # compute the prediction error \n",
    "            Q_sum=0\n",
    "            Q_sum=Q_sum+self.T_bel[s,a,s_next]*(r+max(self.Q[s_next,:])) # for (s,a,s')\n",
    "            \n",
    "            for j in array_rest: # for the rest\n",
    "                Q_sum=Q_sum+self.T_bel[s,a,j]*(self.env.R[j]+max(self.Q[j,:]))\n",
    "                \n",
    "            self.Q_old = self.Q[s,a]\n",
    "            self.Q[s,a] = Q_sum\n",
    "        return self.SPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g): #update all Q(s) when switch reward target\n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        Q_bwd_before = self.Q\n",
    "        for i in range(max(self.env.level),0,-1):\n",
    "            state_ind_set = np.where(self.env.level==i)\n",
    "            for l in range(len(state_ind_set)):\n",
    "                current_S = state_ind_set[l]\n",
    "                for current_A in range(self.env.nA):\n",
    "                    tmp_sum=0\n",
    "                    for j in range(self.env.nS): \n",
    "                        tmp_sum = tmp_sum + self.T_bel[current_S,current_A,j] * \\\n",
    "                                (self.env.R[j]+max(self.Q[j,:]))\n",
    "                    self.Q[current_S,current_A]=tmp_sum\n",
    "        Q_bwd_after = self.Q\n",
    "        self.dQ = Q_bwd_after - Q_bwd_before\n",
    "        self.dQ_bwd_energy = np.sqrt(np.sum(np.sum((self.dQ)**2)))\n",
    "        self.dQ_mean_energy = np.mean(np.mean(self.dQ))\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MB4MDT():\n",
    "    name = 'Model Based'\n",
    "    bnds = [(0,1),(0,5)] #边界\n",
    "    pbnds = [(.1,.5),(.1,2)] #采样边界\n",
    "    p_name   = ['alpha', 'beta']  #参数名\n",
    "    n_params = len(p_name) \n",
    "\n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))] \n",
    "\n",
    "\n",
    "    def __init__(self,env,params):\n",
    "        self.env = env \n",
    "        self._init_mem()\n",
    "        self._init_env_model()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "    \n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "\n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha = params[0]\n",
    "        self.beta  = params[1]\n",
    "    \n",
    "    def _init_env_model(self):\n",
    "        self.T_bel = np.zeros(\n",
    "            [self.env.nS, self.env.nA, self.env.nS] #(s,a,s')\n",
    "            ) \n",
    "        self.T = {0:[[1,2], [6,7], [7,8], [6,5], [6,8]],\n",
    "                  1:[[3,4], [7,8], [6,8], [5,8], [8,5]]}\n",
    "\n",
    "        for a in range(self.env.nA):\n",
    "            for s in range(len(self.T[a])):\n",
    "                self.T_bel[s,a,self.T[a][s]]=[0.5,1-0.5]        \n",
    "        \n",
    "    def _init_critic(self):\n",
    "        self.Q = np.zeros([self.env.nS, self.env.nA])\n",
    "\n",
    "    # ----------- decision ----------- #\n",
    "\n",
    "    def policy(self, s):\n",
    "        q = self.Q[s,:]\n",
    "        return softmax(self.beta*q)\n",
    "\n",
    "    def eval_act(self, s, a):\n",
    "        '''Evaluate the probability of given state and action\n",
    "        '''\n",
    "        logit = self.Q[s, :] \n",
    "        prob  = softmax(self.beta*logit)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    # ----------- learning ----------- #\n",
    "    def learn(self):\n",
    "\n",
    "        s, a, s_next, r, g = self.mem.sample(\n",
    "                        's','a','s_next','r','g')\n",
    "        \n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        if s < 5: \n",
    "            self.SPE=1-self.T_bel[s,a,s_next]\n",
    "            # T-update (increase T(s,a,s') and decrease T(s,a,-) to ensure the sum=1\n",
    "            self.T_bel[s,a,s_next]= self.T_bel[s,a,s_next] + self.alpha * self.SPE\n",
    "            s_unchosen = [elem for elem in self.T[a][s] if elem != s_next] # the rest of the states\n",
    "            for j in s_unchosen:\n",
    "                self.T_bel[s,a,j] = self.T_bel[s,a,j]*(1-self.alpha) #SPE = 0-self.T\n",
    "\n",
    "            # Q update\n",
    "            Q_sum=0\n",
    "            Q_sum=Q_sum+self.T_bel[s,a,s_next]*(r+max(self.Q[s_next,:])) # for (s,a,s')\n",
    "            \n",
    "            for j in s_unchosen: # for the rest\n",
    "                Q_sum=Q_sum+self.T_bel[s,a,j]*(self.env.R[j]+max(self.Q[j,:]))\n",
    "                \n",
    "            self.Q_old = self.Q[s,a] \n",
    "            self.Q[s,a]= Q_sum\n",
    "\n",
    "            #print(SPE, s, a, r, self.alpha, self.Q_old, self.Q[s, a])\n",
    "        return self.SPE,self.Q_old,self.Q\n",
    "    \n",
    "    def bw_update(self,g): #update all Q(s) when switch reward target\n",
    "        self.env.set_reward(g)\n",
    "\n",
    "        Q_bwd_before = self.Q\n",
    "        for i in range(max(self.env.level),0,-1):\n",
    "            state_ind_set = np.where(self.env.level==i)\n",
    "            for l in range(len(state_ind_set)):\n",
    "                current_S = state_ind_set[l]\n",
    "                for current_A in range(self.env.nA):\n",
    "                    tmp_sum=0\n",
    "                    for j in range(self.env.nS): \n",
    "                        tmp_sum = tmp_sum + self.T_bel[current_S,current_A,j] * \\\n",
    "                                (self.env.R[j]+max(self.Q[j,:]))\n",
    "                    self.Q[current_S,current_A]=tmp_sum\n",
    "        Q_bwd_after = self.Q\n",
    "        self.dQ = Q_bwd_after - Q_bwd_before\n",
    "        self.dQ_bwd_energy = np.sqrt(np.sum(np.sum((self.dQ)**2)))\n",
    "        self.dQ_mean_energy = np.mean(np.mean(self.dQ))\n",
    "        return self.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
