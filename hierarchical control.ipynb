{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd958a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_, max_ = 1e-12, 1e12\n",
    "sigmoid = lambda x: 1.0 / (1.0 + clip_exp(-x))\n",
    "logit   = lambda p: np.log(p) - np.log1p(-p)\n",
    "\n",
    "def clip_exp(x):\n",
    "    x = np.clip(x, a_min=-max_, a_max=50)\n",
    "    y = np.exp(x)\n",
    "    return np.where(y > 1e-11, y, 0)\n",
    "\n",
    "class simpleBuffer:\n",
    "    '''Simple Buffer 2.0\n",
    "    Update log: \n",
    "        To prevent naive writing mistakes,\n",
    "        we turn the list storage into dict.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.m = {}\n",
    "        \n",
    "    def push(self, m_dict):\n",
    "        self.m = {k: m_dict[k] for k in m_dict.keys()}\n",
    "        \n",
    "    def sample(self, *args):\n",
    "        lst = [self.m[k] for k in args]\n",
    "        if len(lst)==1: return lst[0]\n",
    "        else: return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HC():\n",
    "    name = 'Hierarchical Control'\n",
    "    bnds = [(0,1),(0,5),(0,1),(0,5)]\n",
    "    pbnds = [(.1,.5),(.1,2),(.1,.5),(.1,2)]\n",
    "    p_name = ['alpha_goal', 'beta_goal', 'alpha_action', 'beta_action']\n",
    "    n_params = len(p_name)\n",
    "    \n",
    "    p_trans = [lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x),\n",
    "               lambda x: 0.0 + (1 - 0.0) * sigmoid(x),   \n",
    "               lambda x: 0.0 + (5 - 0.0) * sigmoid(x)]  \n",
    "    p_links = [lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_)),\n",
    "               lambda y: logit(np.clip((y - 0.0) / (1 - 0.0), eps_, 1 - eps_)),  \n",
    "               lambda y: logit(np.clip((y - 0.0) / (5 - 0.0), eps_, 1 - eps_))]\n",
    "    \n",
    "    def __init__(self, env, params):\n",
    "        self.env = env \n",
    "        self._init_mem()\n",
    "        self._init_critic()\n",
    "        self._load_params(params)\n",
    "        self.current_goal = None\n",
    "    \n",
    "    def _init_mem(self):\n",
    "        self.mem = simpleBuffer()\n",
    "    \n",
    "    def _load_params(self, params):\n",
    "        params = [fn(p) for p, fn in zip(params, self.p_trans)]\n",
    "        self.alpha_goal = params[0]\n",
    "        self.beta_goal  = params[1]\n",
    "        self.alpha_action = params[2]\n",
    "        self.beta_action  = params[3]\n",
    "    \n",
    "    def _init_critic(self):\n",
    "        self.Q_goal = np.zeros([self.env.nS, self.env.nS])\n",
    "        self.Q_action = np.zeros([self.env.nS, self.env.nA, self.env.nS])\n",
    "    \n",
    "    def select_goal(self, s):\n",
    "        q_goal = self.Q_goal[s, :]\n",
    "        return softmax(self.beta_goal * q_goal)\n",
    "    \n",
    "    def policy(self, s):\n",
    "        if self.current_goal is None:\n",
    "            goal_probs = self.select_goal(s)\n",
    "            self.current_goal = np.random.choice(self.env.nS, p=goal_probs)\n",
    "        \n",
    "        q_action = self.Q_action[s, :, self.current_goal]\n",
    "        return softmax(self.beta_action * q_action)\n",
    "    \n",
    "    def eval_act(self, s, a):\n",
    "        if self.current_goal is None:\n",
    "            goal_probs = self.select_goal(s)\n",
    "            self.current_goal = np.random.choice(self.env.nS, p=goal_probs)\n",
    "        \n",
    "        q_action = self.Q_action[s, :, self.current_goal]\n",
    "        prob = softmax(self.beta_action * q_action)\n",
    "        return prob[int(a)]\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, s_next, r, g = self.mem.sample(\n",
    "            's', 'a', 's_next', 'r', 'g')\n",
    "        \n",
    "        self.env.set_reward(g)\n",
    "        \n",
    "        if s < 5:\n",
    "            if self.current_goal is None:\n",
    "                self.current_goal = g\n",
    "            \n",
    "            goal_reached = (s_next == self.current_goal)\n",
    "            \n",
    "            if goal_reached:\n",
    "                intrinsic_reward = 1.0\n",
    "            else:\n",
    "                intrinsic_reward = 0.0\n",
    "            \n",
    "            self.Q_action_old = self.Q_action[s, a, self.current_goal]\n",
    "            action_target = intrinsic_reward + max(self.Q_action[s_next, :, self.current_goal])\n",
    "            self.action_PE = action_target - self.Q_action[s, a, self.current_goal]\n",
    "            self.Q_action[s, a, self.current_goal] += self.alpha_action * self.action_PE\n",
    "            \n",
    "            self.Q_goal_old = self.Q_goal[s, self.current_goal]\n",
    "            goal_target = r + max(self.Q_goal[s_next, :])\n",
    "            self.goal_PE = goal_target - self.Q_goal[s, self.current_goal]\n",
    "            self.Q_goal[s, self.current_goal] += self.alpha_goal * self.goal_PE\n",
    "            \n",
    "            if goal_reached:\n",
    "                self.current_goal = None\n",
    "        \n",
    "        return self.goal_PE, self.Q_goal_old, self.Q_goal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
